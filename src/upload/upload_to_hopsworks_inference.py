import os
import pandas as pd
import hopsworks

# ---------------- SETUP ----------------
project = hopsworks.login()
fs = project.get_feature_store()

# ---------------- CONFIG ----------------
UPLOADS = [
    ("predictions_lgbm_lag28", "citibike_predictions_lag28"),
    ("predictions_lgbm_topk", "citibike_predictions_topk"),
    ("predictions_lgbm_pca", "citibike_predictions_pca"),
    ("future_lgbm_lag28", "citibike_forecast_lag28"),
    ("future_lgbm_topk", "citibike_forecast_topk"),
    ("future_lgbm_pca", "citibike_forecast_pca"),
]

METRICS_PATH = "data/metrics"
PRIMARY_KEYS = ["hour", "station_id"]

# ---------------- FUNCTION ----------------
def upload_csv_group(prefix, feature_group_name):
    matching_files = [
        f for f in os.listdir(METRICS_PATH)
        if f.startswith(prefix) and f.endswith(".csv")
    ]

    if not matching_files:
        print(f"⚠️ No files found for prefix: {prefix}")
        return

    dfs = []
    for file in matching_files:
        station_id = file.split("_")[-1].replace(".csv", "")
        df = pd.read_csv(os.path.join(METRICS_PATH, file), parse_dates=["hour"])
        df["station_id"] = station_id
        dfs.append(df)

    combined = pd.concat(dfs).sort_values(["station_id", "hour"])
    print(f"⬆️ Uploading {prefix} → Feature Group: {feature_group_name}")

    fg = fs.get_or_create_feature_group(
        name=feature_group_name,
        version=1,
        primary_key=PRIMARY_KEYS,
        event_time="hour",
        description=f"{prefix} predictions generated by inference workflow"
    )

    fg.insert(combined, write_options={"wait_for_job": True, "write_mode": "overwrite"})
    print(f"✅ Uploaded to {feature_group_name}\n")

# ---------------- MAIN ----------------
for prefix, fg_name in UPLOADS:
    upload_csv_group(prefix, fg_name)
